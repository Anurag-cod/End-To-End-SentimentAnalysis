[ 2026-01-31 16:17:46,749 ] numexpr.utils - INFO - NumExpr defaulting to 8 threads.
[ 2026-01-31 16:17:52,430 ] tensorflow - DEBUG - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
[ 2026-01-31 16:17:52,888 ] h5py._conv - DEBUG - Creating converter from 7 to 5
[ 2026-01-31 16:17:52,888 ] h5py._conv - DEBUG - Creating converter from 5 to 7
[ 2026-01-31 16:17:52,888 ] h5py._conv - DEBUG - Creating converter from 7 to 5
[ 2026-01-31 16:17:52,888 ] h5py._conv - DEBUG - Creating converter from 5 to 7
[ 2026-01-31 16:17:53,640 ] matplotlib - DEBUG - matplotlib data path: /opt/anaconda3/lib/python3.12/site-packages/matplotlib/mpl-data
[ 2026-01-31 16:17:53,644 ] matplotlib - DEBUG - CONFIGDIR=/Users/anuragramteke/.matplotlib
[ 2026-01-31 16:17:53,645 ] matplotlib - DEBUG - interactive is False
[ 2026-01-31 16:17:53,645 ] matplotlib - DEBUG - platform is darwin
[ 2026-01-31 16:17:53,672 ] matplotlib - DEBUG - CACHEDIR=/Users/anuragramteke/.matplotlib
[ 2026-01-31 16:17:53,673 ] matplotlib.font_manager - DEBUG - Using fontManager instance from /Users/anuragramteke/.matplotlib/fontlist-v330.json
[ 2026-01-31 16:18:06,007 ] root - INFO - Entered the run_pipeline method of PredictionPipeline class
[ 2026-01-31 16:18:06,008 ] root - INFO - Entered the get_model_from_gcloud method of PredictionPipeline class
[ 2026-01-31 16:18:14,407 ] root - INFO - Exited the get_model_from_gcloud method of PredictionPipeline class
[ 2026-01-31 16:18:14,410 ] root - INFO - Running the predict function
[ 2026-01-31 16:18:14,440 ] h5py._conv - DEBUG - Creating converter from 3 to 5
[ 2026-01-31 16:18:14,794 ] absl - WARNING - Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.
[ 2026-01-31 16:18:15,990 ] root - INFO - Prediction: Neutral
[ 2026-01-31 16:18:15,992 ] root - INFO - Exited the run_pipeline method of PredictionPipeline class
[ 2026-01-31 16:25:24,946 ] root - INFO - Entered the run_pipeline method of TrainPipeline class
[ 2026-01-31 16:25:24,947 ] root - INFO - Entered the start_data_ingestion method of TrainPipeline class
[ 2026-01-31 16:25:24,947 ] root - INFO - Getting the data from GCLoud Storage bucket
[ 2026-01-31 16:25:24,948 ] root - INFO - Entered the initiate_data_ingestion method of Data ingestion class
[ 2026-01-31 16:25:24,948 ] root - INFO - Entered the get_data_from_gcloud method of Data ingestion class
[ 2026-01-31 16:25:28,532 ] root - INFO - Exited the get_data_from_gcloud method of Data ingestion class
[ 2026-01-31 16:25:28,536 ] root - INFO - Fetched the data from gcloud bucket
[ 2026-01-31 16:25:28,536 ] root - INFO - Entered the unzip_and_clean method of Data ingestion class
[ 2026-01-31 16:25:29,109 ] root - INFO - Exited the unzip_and_clean method of Data ingestion class
[ 2026-01-31 16:25:29,109 ] root - INFO - Unzipped and prepared training and validation files
[ 2026-01-31 16:25:29,109 ] root - INFO - Exited the initiate_data_ingestion method of Data ingestion class
[ 2026-01-31 16:25:29,109 ] root - INFO - Data ingestion artifact: DataIngestionArtifacts(training_file_path='/Users/anuragramteke/Desktop/NLPProject/artifacts/01_31_2026_16_17_46/DataIngestionArtifacts/twitter_training.csv', validation_file_path='/Users/anuragramteke/Desktop/NLPProject/artifacts/01_31_2026_16_17_46/DataIngestionArtifacts/twitter_validation.csv')
[ 2026-01-31 16:25:29,109 ] root - INFO - Got the train and valid from GCLoud Storage
[ 2026-01-31 16:25:29,109 ] root - INFO - Exited the start_data_ingestion method of TrainPipeline class
[ 2026-01-31 16:25:29,109 ] root - INFO - Entered the start_data_transformation method of TrainPipeline class
[ 2026-01-31 16:25:29,109 ] root - INFO - Entered the initiate_data_transformation method of Data transformation class
[ 2026-01-31 16:25:29,109 ] root - INFO - Entered load_and_prepare (notebook: training + test, drop Header1 & company)
[ 2026-01-31 16:25:29,266 ] root - INFO - Combined and cleaned dataframe shape: (70252, 2)
[ 2026-01-31 16:25:29,267 ] root - INFO - Applying process_text (lemmatization, stopwords, etc.) on text column
[ 2026-01-31 16:25:38,221 ] root - INFO - Returning the DataTransformationArtifacts
[ 2026-01-31 16:25:38,224 ] root - INFO - Exited the start_data_transformation method of TrainPipeline class
[ 2026-01-31 16:25:38,224 ] root - INFO - Entered the start_model_trainer method of TrainPipeline class
[ 2026-01-31 16:25:38,224 ] root - INFO - Entered initiate_model_trainer method of ModelTrainer class
[ 2026-01-31 16:25:38,224 ] root - INFO - Entered the split_data function
[ 2026-01-31 16:25:38,274 ] root - INFO - Exited the split_data function
[ 2026-01-31 16:25:38,274 ] root - INFO - Applying tokenization and padding (notebook: max_vocab=20000, maxlen=100)
[ 2026-01-31 16:25:39,115 ] root - INFO - Vocab size (for Embedding): 24150
[ 2026-01-31 16:25:39,338 ] root - INFO - Starting model training (notebook: 20 epochs)
